{"cells":[{"cell_type":"markdown","id":"4b3affd1","metadata":{"id":"4b3affd1"},"source":["## Implementation of Transformer using pytorch"]},{"cell_type":"code","execution_count":1,"id":"2c5f9bca","metadata":{"id":"2c5f9bca","executionInfo":{"status":"ok","timestamp":1713993575695,"user_tz":-330,"elapsed":3956,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import math\n","import copy"]},{"cell_type":"code","execution_count":2,"id":"3e0dff3b","metadata":{"id":"3e0dff3b","executionInfo":{"status":"ok","timestamp":1713993575696,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        if mask is not None:\n","            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n","        attn_probs = torch.softmax(attn_scores, dim=-1)\n","        output = torch.matmul(attn_probs, V)\n","        return output\n","\n","    def split_heads(self, x):\n","        batch_size, seq_length, d_model = x.size()\n","        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        batch_size, _, seq_length, d_k = x.size()\n","        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        Q = self.split_heads(self.W_q(Q))\n","        K = self.split_heads(self.W_k(K))\n","        V = self.split_heads(self.W_v(V))\n","\n","        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n","        output = self.W_o(self.combine_heads(attn_output))\n","        return output\n"]},{"cell_type":"code","execution_count":3,"id":"ec626725","metadata":{"id":"ec626725","executionInfo":{"status":"ok","timestamp":1713993575696,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["class PositionWiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))"]},{"cell_type":"code","execution_count":4,"id":"76a09d89","metadata":{"id":"76a09d89","executionInfo":{"status":"ok","timestamp":1713993575696,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_seq_length):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_seq_length, d_model)\n","        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]"]},{"cell_type":"code","execution_count":5,"id":"39285ccb","metadata":{"id":"39285ccb","executionInfo":{"status":"ok","timestamp":1713993576431,"user_tz":-330,"elapsed":737,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        attn_output = self.self_attn(x, x, x, mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x"]},{"cell_type":"code","execution_count":6,"id":"8721d6a6","metadata":{"id":"8721d6a6","executionInfo":{"status":"ok","timestamp":1713993576432,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_output, src_mask, tgt_mask):\n","        attn_output = self.self_attn(x, x, x, tgt_mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n","        x = self.norm2(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm3(x + self.dropout(ff_output))\n","        return x"]},{"cell_type":"code","execution_count":7,"id":"e1b2b005","metadata":{"id":"e1b2b005","executionInfo":{"status":"ok","timestamp":1713993576432,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n","        super(Transformer, self).__init__()\n","        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n","\n","        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","\n","        self.fc = nn.Linear(d_model, tgt_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def generate_mask(self, src, tgt):\n","        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n","        seq_length = tgt.size(1)\n","        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n","        tgt_mask = tgt_mask & nopeak_mask\n","        return src_mask, tgt_mask\n","\n","    def forward(self, src, tgt):\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n","        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n","\n","        enc_output = src_embedded\n","        for enc_layer in self.encoder_layers:\n","            enc_output = enc_layer(enc_output, src_mask)\n","\n","        dec_output = tgt_embedded\n","        for dec_layer in self.decoder_layers:\n","            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n","\n","        output = self.fc(dec_output)\n","        return output"]},{"cell_type":"code","execution_count":8,"id":"6aec0e8c","metadata":{"id":"6aec0e8c","executionInfo":{"status":"ok","timestamp":1713993577017,"user_tz":-330,"elapsed":589,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[],"source":["src_vocab_size = 5000\n","tgt_vocab_size = 5000\n","d_model = 512\n","num_heads = 8\n","num_layers = 6\n","d_ff = 2048\n","max_seq_length = 100\n","dropout = 0.1\n","\n","transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n","\n","# Generate random sample data\n","src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n","tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"]},{"cell_type":"code","execution_count":9,"id":"cd92d1d3","metadata":{"id":"cd92d1d3","outputId":"8b4c77ad-0d31-4ee9-a95b-96900ffdf49f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713995689439,"user_tz":-330,"elapsed":2112424,"user":{"displayName":"Anubhav Bhoumik","userId":"18207956708162701166"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 8.684225082397461\n","Epoch: 2, Loss: 8.55360221862793\n","Epoch: 3, Loss: 8.477655410766602\n","Epoch: 4, Loss: 8.426728248596191\n","Epoch: 5, Loss: 8.36938190460205\n","Epoch: 6, Loss: 8.304092407226562\n","Epoch: 7, Loss: 8.220489501953125\n","Epoch: 8, Loss: 8.140716552734375\n","Epoch: 9, Loss: 8.057660102844238\n","Epoch: 10, Loss: 7.976222038269043\n","Epoch: 11, Loss: 7.8934102058410645\n","Epoch: 12, Loss: 7.810605049133301\n","Epoch: 13, Loss: 7.729732513427734\n","Epoch: 14, Loss: 7.652928352355957\n","Epoch: 15, Loss: 7.560025215148926\n","Epoch: 16, Loss: 7.480445861816406\n","Epoch: 17, Loss: 7.3939924240112305\n","Epoch: 18, Loss: 7.310922145843506\n","Epoch: 19, Loss: 7.232973098754883\n","Epoch: 20, Loss: 7.149237155914307\n","Epoch: 21, Loss: 7.070291519165039\n","Epoch: 22, Loss: 6.995792388916016\n","Epoch: 23, Loss: 6.922571182250977\n","Epoch: 24, Loss: 6.842485427856445\n","Epoch: 25, Loss: 6.763674259185791\n","Epoch: 26, Loss: 6.690556049346924\n","Epoch: 27, Loss: 6.615949630737305\n","Epoch: 28, Loss: 6.543013572692871\n","Epoch: 29, Loss: 6.471689701080322\n","Epoch: 30, Loss: 6.40070915222168\n","Epoch: 31, Loss: 6.328438758850098\n","Epoch: 32, Loss: 6.261263847351074\n","Epoch: 33, Loss: 6.186485767364502\n","Epoch: 34, Loss: 6.130897521972656\n","Epoch: 35, Loss: 6.0619001388549805\n","Epoch: 36, Loss: 5.997175693511963\n","Epoch: 37, Loss: 5.932234764099121\n","Epoch: 38, Loss: 5.873751163482666\n","Epoch: 39, Loss: 5.803571701049805\n","Epoch: 40, Loss: 5.751322269439697\n","Epoch: 41, Loss: 5.671782970428467\n","Epoch: 42, Loss: 5.620633602142334\n","Epoch: 43, Loss: 5.552824974060059\n","Epoch: 44, Loss: 5.492761611938477\n","Epoch: 45, Loss: 5.432076930999756\n","Epoch: 46, Loss: 5.374764442443848\n","Epoch: 47, Loss: 5.31240177154541\n","Epoch: 48, Loss: 5.257850170135498\n","Epoch: 49, Loss: 5.200689315795898\n","Epoch: 50, Loss: 5.1418070793151855\n","Epoch: 51, Loss: 5.087031364440918\n","Epoch: 52, Loss: 5.036418437957764\n","Epoch: 53, Loss: 4.969945907592773\n","Epoch: 54, Loss: 4.917186737060547\n","Epoch: 55, Loss: 4.868462085723877\n","Epoch: 56, Loss: 4.807980537414551\n","Epoch: 57, Loss: 4.759270668029785\n","Epoch: 58, Loss: 4.708003044128418\n","Epoch: 59, Loss: 4.653700351715088\n","Epoch: 60, Loss: 4.596658229827881\n","Epoch: 61, Loss: 4.54737663269043\n","Epoch: 62, Loss: 4.491761684417725\n","Epoch: 63, Loss: 4.437838554382324\n","Epoch: 64, Loss: 4.389129161834717\n","Epoch: 65, Loss: 4.336400032043457\n","Epoch: 66, Loss: 4.287975311279297\n","Epoch: 67, Loss: 4.247129440307617\n","Epoch: 68, Loss: 4.1976165771484375\n","Epoch: 69, Loss: 4.152222156524658\n","Epoch: 70, Loss: 4.098055362701416\n","Epoch: 71, Loss: 4.046657562255859\n","Epoch: 72, Loss: 3.997037887573242\n","Epoch: 73, Loss: 3.9470863342285156\n","Epoch: 74, Loss: 3.901060104370117\n","Epoch: 75, Loss: 3.8524255752563477\n","Epoch: 76, Loss: 3.8066911697387695\n","Epoch: 77, Loss: 3.7590837478637695\n","Epoch: 78, Loss: 3.7081151008605957\n","Epoch: 79, Loss: 3.6655008792877197\n","Epoch: 80, Loss: 3.6139273643493652\n","Epoch: 81, Loss: 3.568305730819702\n","Epoch: 82, Loss: 3.519589424133301\n","Epoch: 83, Loss: 3.4741053581237793\n","Epoch: 84, Loss: 3.42940354347229\n","Epoch: 85, Loss: 3.385359525680542\n","Epoch: 86, Loss: 3.3386647701263428\n","Epoch: 87, Loss: 3.2930731773376465\n","Epoch: 88, Loss: 3.2549619674682617\n","Epoch: 89, Loss: 3.2059311866760254\n","Epoch: 90, Loss: 3.1646475791931152\n","Epoch: 91, Loss: 3.126539945602417\n","Epoch: 92, Loss: 3.0706121921539307\n","Epoch: 93, Loss: 3.0329697132110596\n","Epoch: 94, Loss: 2.989110231399536\n","Epoch: 95, Loss: 2.9473063945770264\n","Epoch: 96, Loss: 2.9065403938293457\n","Epoch: 97, Loss: 2.857273817062378\n","Epoch: 98, Loss: 2.814192295074463\n","Epoch: 99, Loss: 2.771514654159546\n","Epoch: 100, Loss: 2.740572929382324\n"]}],"source":["criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","\n","transformer.train()\n","\n","for epoch in range(100):\n","    optimizer.zero_grad()\n","    output = transformer(src_data, tgt_data[:, :-1])\n","    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}